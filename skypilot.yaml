resources:
  accelerators: H100

num_nodes: 1

workdir: ./auth

envs:
  BASE_MODEL: Qwen/Qwen3-8B
  HF_HUB_ENABLE_HF_TRANSFER: "1"
  CUDA_HOME: "/usr/local/cuda-12"
  LD_LIBRARY_PATH: "/usr/local/cuda-12/lib64:$LD_LIBRARY_PATH"

secrets:
  HF_TOKEN: null
  SERVER_API_KEY: null

setup: |
  # Disable Conda environment
  echo "Disabling Conda environment..."
  conda deactivate

  # Install system dependencies
  apt update -y
  apt install -y --no-install-recommends curl unzip numactl ninja-build
  
  # Install package managers
  curl -fsSL https://astral.sh/uv/install.sh | sh
  export PATH="/root/.local/bin/:$PATH"
  
  curl -fsSL https://bun.sh/install | bash -s "bun-v1.2.18"
  # sudo snap install bun-js # Hyperbolic
  export PATH="/root/.bun/bin:$PATH"
  
  # Create virtual environment with uv
  uv venv .venv --python 3.10.12
  source .venv/bin/activate
  
  # Install Python dependencies
  uv pip install sentencepiece
  uv pip install "sglang[all]>=0.4.9.post1"
  uv pip install flashinfer-python -i https://flashinfer.ai/whl/cu126/torch2.6
  
  # Install Bun dependencies
  bun i -p

run: |
  # Disable Conda environment
  conda deactivate
  
  # Activate virtual environment
  source .venv/bin/activate
  export PATH="/root/.local/bin/:$PATH"
  export PATH="/root/.bun/bin:$PATH"
  
  # Start sglang server in background
  echo 'Starting SGLang OpenAI API server and Bun web server...'
  python -m sglang.launch_server \
    --model-path $BASE_MODEL \
    --enable-lora \
    --lora-paths lora0=RubricLabs/Qwen3-8B-uirl \
    --disable-radix-cache \
    --grammar-backend llguidance \
    --reasoning-parser qwen3 \
    --host 0.0.0.0 \
    --port 8000 & \
    bun index.ts
